{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "Text to feature using TF-IDF.\n",
    "## Solution\n",
    "Term frequency (TF)（词频）: Term frequency is simply the ratio of the count of a\n",
    "word present in a sentence, to the length of the sentence.\n",
    "(一个词在该句子中出现的次数与句子长度的比例)\n",
    "\n",
    "TF is basically capturing the importance of the word irrespective of the\n",
    "length of the document. For example, a word with the frequency of 3 with\n",
    "the length of sentence being 10 is not the same as when the word length of\n",
    "sentence is 100 words. It should get more importance in the first scenario;\n",
    "that is what TF does.\n",
    "\n",
    "Inverse Document Frequency (IDF): IDF of each word is the log of\n",
    "the ratio of the total number of rows to the number of rows in a particular\n",
    "document in which that word is present.\n",
    "\n",
    "IDF = log(N/n), where N is the total number of rows and n is the\n",
    "number of rows in which the word was present.\n",
    "\n",
    "IDF will measure the rareness of a term. Words like “a,” and “the” show\n",
    "up in all the documents of the corpus, but rare words will not be there\n",
    "in all the documents. So, if a word is appearing in almost all documents,\n",
    "then that word is of no use to us since it is not helping to classify or in\n",
    "information retrieval. IDF will nullify this problem.\n",
    "\n",
    "TF-IDF is the simple product of TF and IDF so that both of the\n",
    "drawbacks are addressed, which makes predictions and information\n",
    "retrieval relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6-1 Read the text data\n",
    "A familiar phrase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"The dog.\",\n",
    "    \"The fox\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6-2 Creating the Features\n",
    "Execute the below code on the text data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Tokenize and build vocab\n",
    "vectorizer.fit(Text)\n",
    "\n",
    "#Summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa ccc aaa aaa', 'aaa aaa', 'aaa aaa aaa', 'aaa aaa aaa aaa', 'aaa bbb aaa bbb aaa', 'ccc aaa aaa ccc aaa']\n",
      "['aaa', 'bbb', 'ccc']\n",
      "[[3 0 1]\n",
      " [2 0 0]\n",
      " [3 0 0]\n",
      " [4 0 0]\n",
      " [3 2 0]\n",
      " [3 0 2]]\n",
      "  (0, 2)\t0.5243329281310096\n",
      "  (0, 0)\t0.85151334721046\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 1)\t0.8323642772534078\n",
      "  (4, 0)\t0.5542289327998063\n",
      "  (5, 2)\t0.7763051366495072\n",
      "  (5, 0)\t0.63035730725644\n",
      "[[0.85151335 0.         0.52433293]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.55422893 0.83236428 0.        ]\n",
      " [0.63035731 0.         0.77630514]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "\n",
    "corpus = ['aaa ccc aaa aaa', \n",
    "          'aaa aaa', \n",
    "          'aaa aaa aaa', \n",
    "          'aaa aaa aaa aaa',\n",
    "          'aaa bbb aaa bbb aaa',\n",
    "          'ccc aaa aaa ccc aaa'\n",
    "         ]\n",
    "print(corpus)\n",
    "vectorizer = CountVectorizer() \n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 获取词袋模型中的所有词语   \n",
    "word = vectorizer.get_feature_names()  \n",
    "print(word) \n",
    "\n",
    "# 获取每个词在该行（文档）中出现的次数\n",
    "counts =  X.toarray()\n",
    "print (counts)\n",
    "\n",
    "# # 第一种方法\n",
    "# transformer = TfidfTransformer()\n",
    "# tfidf = transformer.fit_transform(X)\n",
    "# #tfidf = transformer.fit_transform(counts) #与上一行的效果完全一样\n",
    "\n",
    "\n",
    "#第二种方法\n",
    "transformer = TfidfVectorizer()\n",
    "transformer.fit(corpus)\n",
    "tfidf = transformer.transform(corpus)\n",
    "\n",
    "print(tfidf)\n",
    "print(tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
