{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题\n",
    "\n",
    "读取HTML文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1 安装并导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\workspace\\python\\anaconda3\\lib\\site-packages (from bs4) (4.6.0)\n",
      "Building wheels for collected packages: bs4\n",
      "  Running setup.py bdist_wheel for bs4: started\n",
      "  Running setup.py bdist_wheel for bs4: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\fucheng\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2 抓取HTML文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3 分析HTML文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Natural language processing - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );\n",
      "  </script>\n",
      "  <script>\n",
      "   (window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":888277178,\"wgRevisionId\":888277178,\"wgArticleId\":21652,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Webarchive template wayback links\",\"All accuracy disputes\",\"Articles with disputed statements from June 2018\",\"Wikipedia articles with LCCN identifiers\",\"Wikipedia articles with NDL identifiers\",\"Natural language processing\",\"Computational linguistics\",\"Speech\n"
     ]
    }
   ],
   "source": [
    "#Parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "# Print few lines\n",
    "print (strhtm[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4 抽取标签值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Natural language processing - Wikipedia</title>\n",
      "Natural language processing - Wikipedia\n",
      "None\n",
      "Natural language processing\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.a.string)\n",
    "print(soup.b.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5 抽取实例和特定的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Jump to navigation\n",
      "Jump to search\n",
      "Language processing in the brain\n",
      "None\n",
      "None\n",
      "automated online assistant\n",
      "customer service\n",
      "[1]\n",
      "computer science\n",
      "information engineering\n",
      "artificial intelligence\n",
      "natural language\n",
      "speech recognition\n",
      "natural language understanding\n",
      "natural language generation\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "edit\n",
      "history of natural language processing\n",
      "Alan Turing\n",
      "Intelligence\n",
      "Turing test\n",
      "Georgetown experiment\n",
      "automatic translation\n",
      "[2]\n",
      "ALPAC report\n",
      "statistical machine translation\n",
      "SHRDLU\n",
      "blocks worlds\n",
      "ELIZA\n",
      "Rogerian psychotherapist\n",
      "Joseph Weizenbaum\n",
      "ontologies\n",
      "chatterbots\n",
      "PARRY\n",
      "Racter\n",
      "Jabberwacky\n",
      "machine learning\n",
      "Moore's law\n",
      "Chomskyan\n",
      "transformational grammar\n",
      "corpus linguistics\n",
      "[3]\n",
      "decision trees\n",
      "part-of-speech tagging\n",
      "hidden Markov models\n",
      "statistical models\n",
      "probabilistic\n",
      "real-valued\n",
      "cache language models\n",
      "speech recognition\n",
      "machine translation\n",
      "textual corpora\n",
      "Parliament of Canada\n",
      "European Union\n",
      "unsupervised\n",
      "semi-supervised learning\n",
      "supervised learning\n",
      "World Wide Web\n",
      "time complexity\n",
      "representation learning\n",
      "deep neural network\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "word embeddings\n",
      "neural machine translation\n",
      "statistical machine translation\n",
      "edit\n",
      "[9]\n",
      "[10]\n",
      "stemming\n",
      "[11]\n",
      "[12]\n",
      "machine learning\n",
      "statistical inference\n",
      "corpora\n",
      "decision trees\n",
      "statistical models\n",
      "probabilistic\n",
      "real-valued\n",
      "edit\n",
      "edit\n",
      "Grammar induction\n",
      "[13]\n",
      "formal grammar\n",
      "Lemmatization\n",
      "Morphological segmentation\n",
      "morphemes\n",
      "morphology\n",
      "English\n",
      "inflectional morphology\n",
      "Turkish\n",
      "Meitei\n",
      "[14]\n",
      "agglutinated\n",
      "Part-of-speech tagging\n",
      "part of speech\n",
      "parts of speech\n",
      "noun\n",
      "verb\n",
      "noun\n",
      "verb\n",
      "adjective\n",
      "dubious\n",
      "discuss\n",
      "inflectional morphology\n",
      "English\n",
      "Chinese\n",
      "tonal language\n",
      "Parsing\n",
      "Stochastic grammar\n",
      "parse tree\n",
      "grammar\n",
      "natural languages\n",
      "ambiguous\n",
      "Probabilistic Context-Free Grammar\n",
      "Sentence breaking\n",
      "sentence boundary disambiguation\n",
      "periods\n",
      "punctuation marks\n",
      "abbreviations\n",
      "Stemming\n",
      "Word segmentation\n",
      "English\n",
      "Chinese\n",
      "Japanese\n",
      "Thai\n",
      "vocabulary\n",
      "morphology\n",
      "Terminology extraction\n",
      "edit\n",
      "Lexical semantics\n",
      "Machine translation\n",
      "AI-complete\n",
      "Named entity recognition\n",
      "capitalization\n",
      "Chinese\n",
      "Arabic\n",
      "German\n",
      "nouns\n",
      "French\n",
      "Spanish\n",
      "adjectives\n",
      "Natural language generation\n",
      "Natural language understanding\n",
      "first-order logic\n",
      "computer\n",
      "closed-world assumption\n",
      "open-world assumption\n",
      "[15]\n",
      "Optical character recognition\n",
      "Question answering\n",
      "[16]\n",
      "Recognizing Textual entailment\n",
      "[17]\n",
      "Relationship extraction\n",
      "Sentiment analysis\n",
      "multimodal sentiment analysis\n",
      "Topic segmentation\n",
      "Word sense disambiguation\n",
      "meaning\n",
      "WordNet\n",
      "edit\n",
      "Automatic summarization\n",
      "Coreference resolution\n",
      "Anaphora resolution\n",
      "pronouns\n",
      "referring expressions\n",
      "Discourse analysis\n",
      "discourse\n",
      "speech acts\n",
      "edit\n",
      "Speech recognition\n",
      "text to speech\n",
      "AI-complete\n",
      "natural speech\n",
      "speech segmentation\n",
      "coarticulation\n",
      "analog signal\n",
      "Speech segmentation\n",
      "speech recognition\n",
      "Text-to-speech\n",
      "[18]\n",
      "Dialogue\n",
      "edit\n",
      "1 the Road\n",
      "edit\n",
      "Association for Computational Linguistics\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "edit\n",
      "1 the Road\n",
      "Automated essay scoring\n",
      "Biomedical text mining\n",
      "Compound term processing\n",
      "Computational linguistics\n",
      "Computer-assisted reviewing\n",
      "Controlled natural language\n",
      "Deep learning\n",
      "Deep linguistic processing\n",
      "Foreign language reading aid\n",
      "Foreign language writing aid\n",
      "Information extraction\n",
      "Information retrieval\n",
      "Language and Communication Technologies\n",
      "Language technology\n",
      "Latent Dirichlet allocation (LDA)\n",
      "Latent semantic indexing\n",
      "List of natural language processing toolkits\n",
      "Naomi Sager\n",
      "Native-language identification\n",
      "Natural language programming\n",
      "Natural language search\n",
      "Query expansion\n",
      "Reification (linguistics)\n",
      "Semantic folding\n",
      "Speech processing\n",
      "Spoken dialogue system\n",
      "Text-proofing\n",
      "Text simplification\n",
      "Thought vector\n",
      "Truecasing\n",
      "Question answering\n",
      "Word2vec\n",
      "edit\n",
      "^\n",
      "Implementing an online help desk system based on conversational agent\n",
      "ISBN\n",
      "978-1-60558-829-2\n",
      "doi\n",
      "10.1145/1643823.1643908\n",
      "^\n",
      "\"The history of machine translation in a nutshell\"\n",
      "self-published source\n",
      "^\n",
      "corner cases\n",
      "pathological\n",
      "thought experiments\n",
      "corpus linguistics\n",
      "corpora\n",
      "poverty of the stimulus\n",
      "^\n",
      "A Primer on Neural Network Models for Natural Language Processing\n",
      "^\n",
      "http://www.deeplearningbook.org/\n",
      "^\n",
      "https://arxiv.org/abs/1602.02410\n",
      "^\n",
      "https://aclanthology.coli.uni-saarland.de/papers/D16-1257/d16-1257\n",
      "^\n",
      "https://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf\n",
      "^\n",
      "http://hci.stanford.edu/winograd/shrdlu/\n",
      "^\n",
      "^\n",
      "Mark Johnson. How the statistical revolution changes (computational) linguistics.\n",
      "^\n",
      "Philip Resnik. Four revolutions.\n",
      "^\n",
      "Natural language grammar induction using a constituent-context model\n",
      "^\n",
      "Manipuri Morpheme Identification\n",
      "^\n",
      "Formalizing Semantic of Natural Language through Conceptualization from Existence\n",
      "Archived\n",
      "Wayback Machine\n",
      "^\n",
      "Versatile question answering systems: seeing in synthesis\n",
      "^\n",
      "https://tac.nist.gov//2011/RTE/\n",
      "^\n",
      "CiteSeerX\n",
      "10.1.1.668.869\n",
      "doi\n",
      "10.1007/978-3-642-29364-1_2\n",
      "ISBN\n",
      "9783642293634\n",
      "^\n",
      "^\n",
      "\"The Cobbler's Children Won't Go Unshod\"\n",
      "^\n",
      "\"The NLP4NLP Corpus (I): 50 Years of Publication Collaboration and Citation in Speech and Language Processing\"\n",
      "^\n",
      "\"The NLP4NLP Corpus (I): 50 Years of Research in Speech and Language Processing\"\n",
      "edit\n",
      "\"Models of natural language understanding\"\n",
      "doi\n",
      "10.1073/pnas.92.22.9977\n",
      "PMC\n",
      "40721\n",
      "PMID\n",
      "7479812\n",
      "ISBN\n",
      "978-0-596-51649-9\n",
      "ISBN\n",
      "978-0-13-187321-6\n",
      "ISBN\n",
      "978-1848218482\n",
      "ISBN\n",
      "978-1848219212\n",
      "ISBN\n",
      "978-0-521-86571-5\n",
      "Official html and pdf versions available without charge.\n",
      "ISBN\n",
      "978-0-262-13360-9\n",
      "ISBN\n",
      "978-0-387-19557-5\n",
      "Natural language processing\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural language processing\n",
      "Natural language understanding\n",
      "Text corpus\n",
      "Speech corpus\n",
      "Stopwords\n",
      "Bag-of-words\n",
      "AI-complete\n",
      "n-gram\n",
      "Bigram\n",
      "Trigram\n",
      "Text analysis\n",
      "Text segmentation\n",
      "Part-of-speech tagging\n",
      "Text chunking\n",
      "Compound term processing\n",
      "Collocation extraction\n",
      "Stemming\n",
      "Lemmatisation\n",
      "Named-entity recognition\n",
      "Coreference resolution\n",
      "Sentiment analysis\n",
      "Concept mining\n",
      "Parsing\n",
      "Word-sense disambiguation\n",
      "Ontology learning\n",
      "Terminology extraction\n",
      "Truecasing\n",
      "Automatic summarization\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "Machine translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Neural\n",
      "None\n",
      "Speech recognition\n",
      "Speech synthesis\n",
      "Optical character recognition\n",
      "Natural language generation\n",
      "Topic model\n",
      "Pachinko allocation\n",
      "Latent Dirichlet allocation\n",
      "Latent semantic analysis\n",
      "None\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "Grammar checker\n",
      "Predictive text\n",
      "Spell checker\n",
      "Syntax guessing\n",
      "None\n",
      "Automated online assistant\n",
      "Chatbot\n",
      "Interactive fiction\n",
      "Question answering\n",
      "Voice user interface\n",
      "Authority control\n",
      "None\n",
      "LCCN\n",
      "sh88002425\n",
      "NDL\n",
      "00562347\n",
      "None\n",
      "Information technology portal\n",
      "None\n",
      "Language portal\n",
      "None\n",
      "Natural language processing portal\n",
      "https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=888277178\n",
      "Categories\n",
      "Natural language processing\n",
      "Computational linguistics\n",
      "Speech recognition\n",
      "Computational fields of study\n",
      "Artificial intelligence\n",
      "Webarchive template wayback links\n",
      "All accuracy disputes\n",
      "Articles with disputed statements from June 2018\n",
      "Wikipedia articles with LCCN identifiers\n",
      "Wikipedia articles with NDL identifiers\n",
      "Talk\n",
      "Contributions\n",
      "Create account\n",
      "Log in\n",
      "Article\n",
      "Talk\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "None\n",
      "Main page\n",
      "Contents\n",
      "Featured content\n",
      "Current events\n",
      "Random article\n",
      "Donate to Wikipedia\n",
      "Wikipedia store\n",
      "Help\n",
      "About Wikipedia\n",
      "Community portal\n",
      "Recent changes\n",
      "Contact page\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Wikidata item\n",
      "Cite this page\n",
      "Create a book\n",
      "Download as PDF\n",
      "Printable version\n",
      "Wikimedia Commons\n",
      "Afrikaans\n",
      "العربية\n",
      "Azərbaycanca\n",
      "বাংলা\n",
      "Bân-lâm-gú\n",
      "Беларуская\n",
      "Беларуская (тарашкевіца)‎\n",
      "Български\n",
      "Català\n",
      "Čeština\n",
      "Dansk\n",
      "Deutsch\n",
      "Eesti\n",
      "Ελληνικά\n",
      "Español\n",
      "Euskara\n",
      "فارسی\n",
      "Français\n",
      "Galego\n",
      "한국어\n",
      "Հայերեն\n",
      "हिन्दी\n",
      "Bahasa Indonesia\n",
      "Íslenska\n",
      "Italiano\n",
      "עברית\n",
      "ಕನ್ನಡ\n",
      "ქართული\n",
      "Lietuvių\n",
      "Македонски\n",
      "मराठी\n",
      "Монгол\n",
      "မြန်မာဘာသာ\n",
      "日本語\n",
      "ଓଡ଼ିଆ\n",
      "Piemontèis\n",
      "Polski\n",
      "Português\n",
      "Română\n",
      "Русский\n",
      "Simple English\n",
      "کوردی\n",
      "Српски / srpski\n",
      "தமிழ்\n",
      "ไทย\n",
      "Türkçe\n",
      "Українська\n",
      "Tiếng Việt\n",
      "中文\n",
      "Edit links\n",
      "Creative Commons Attribution-ShareAlike License\n",
      "None\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "Wikimedia Foundation, Inc.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Developers\n",
      "Cookie statement\n",
      "Mobile view\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for x in soup.find_all('a'): print(x.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
