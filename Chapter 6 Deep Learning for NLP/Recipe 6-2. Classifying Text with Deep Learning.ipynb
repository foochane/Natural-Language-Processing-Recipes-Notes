{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "We want to build a text classification model using CNN, RNN, and LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-1 Understanding/defining business problem\n",
    "Email classification (spam or ham). We need to classify spam or ham email\n",
    "based on email content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-2 Identifying potential data sources, collection,and understanding\n",
    "Using the same data used in Recipe 4-6 from Chapter 4:\n",
    "\n",
    "Please download data from the below link and save it in your working\n",
    "\n",
    "directory:\n",
    "\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset#spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read file\n",
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "#check sample content in the email\n",
    "file_content['v2'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-3 Text preprocessing\n",
    "Let’s preprocess the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go jurong point, crazy.. Available bugis n gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry 2 wkly comp win FA Cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say early hor... U c already say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I think goes usf, lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go jurong point, crazy.. Available bugis n gre...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry 2 wkly comp win FA Cup final tkts 2...\n",
       "3    ham          U dun say early hor... U c already say...\n",
       "4    ham          Nah I think goes usf, lives around though"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import library\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove stop words\n",
    "stop = stopwords.words('english')\n",
    "file_content['v2'] = file_content['v2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Delete unwanted columns\n",
    "Email_Data = file_content[['v1', 'v2']]\n",
    "\n",
    "# Rename column names\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    go jurong point  crazy   available bugis n gre...\n",
       "1                        ok lar    joking wif u oni   \n",
       "2    free entry 2 wkly comp win fa cup final tkts 2...\n",
       "3            u dun say early hor    u c already say   \n",
       "4            nah i think goes usf  lives around though\n",
       "Name: Email, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete punctuations, convert text in lower case and delete the double space\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:re.sub('[!@#$:).;,?&]', ' ', x.lower()))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x:re.sub(' ', ' ', x))\n",
    "Email_Data['Email'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating text(input) and target classes\n",
    "list_sentences_rawdata = Email_Data[\"Email\"].fillna(\"_na_\").values\n",
    "list_classes = [\"Target\"]\n",
    "target = Email_Data[list_classes].values\n",
    "To_Process=Email_Data[['Email', 'Target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2-4 Data preparation for model building\n",
    "Now we prepare the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7940 unique tokens.\n",
      "(4457, 300)\n",
      "(1115, 300)\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding,Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints,optimizers, layers\n",
    "from keras.layers import Dense, Input, Flatten, Dropout,BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Train and test split with 80:20 ratio\n",
    "train, test = train_test_split(To_Process, test_size=0.2)\n",
    "\n",
    "# Define the sequence lengths, max number of words and embedding dimensions\n",
    "# Sequence length of each sentence. If more, truncate. If less,pad with zeros\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "\n",
    "# Top 20000 frequently occurring words\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# Get the frequently occurring words\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train.Email)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Email)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# get only the top frequent words on train\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# get only the top frequent words on test\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n",
      "(array([0, 1], dtype=int64), array([3876,  581], dtype=int64))\n",
      "(array([0, 1], dtype=int64), array([949, 166], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# converts the character array to numeric array. Assigns levels to unique labels.\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)\n",
    "print(le.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4457, 300)\n",
      "Shape of label tensor: (4457, 2)\n",
      "Shape of label tensor: (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "# changing data types\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-5 Model building and predicting\n",
    "We are building the models using different deep learning approaches\n",
    "like CNN, RNN, LSTM, and Bidirectional LSTM and comparing the\n",
    "performance of each model using different accuracy metrics.\n",
    "\n",
    "We can now define our CNN model.\n",
    "\n",
    "Here we define a single hidden layer with 128 memory units. The\n",
    "network uses a dropout with a probability of 0.5. The output layer is a\n",
    "dense layer using the softmax activation function to output a probability\n",
    "prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN 1D model.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "# import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.layers import Dense, Input, LSTM, Embedding,\n",
    "# Dropout, Activation\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D,\n",
    "# Conv1D, SimpleRNN\n",
    "# from keras.models import Model\n",
    "# from keras.models import Sequential\n",
    "# from keras import initializers, regularizers, constraints,\n",
    "# optimizers, layers\n",
    "# from keras.layers import Dense, Input, Flatten, Dropout,\n",
    "# BatchNormalization\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "# from keras.models import Sequential\n",
    "\n",
    "\n",
    "print('Training CNN 1D model.')\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    "                    EMBEDDING_DIM,\n",
    "                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 300, 32)           14976     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 298, 16)           1552      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                850       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,017,480\n",
      "Trainable params: 2,017,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now fitting our model to the data. Here we have 5 epochs and a\n",
    "batch size of 64 patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3565 samples, validate on 892 samples\n",
      "Epoch 1/5\n",
      "3565/3565 [==============================] - 43s 12ms/step - loss: 1.9176e-04 - acc: 1.0000 - val_loss: 9.8180e-05 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "3565/3565 [==============================] - 44s 12ms/step - loss: 1.4344e-04 - acc: 1.0000 - val_loss: 8.1382e-05 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "3565/3565 [==============================] - 45s 13ms/step - loss: 1.3503e-04 - acc: 1.0000 - val_loss: 7.0436e-05 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "3565/3565 [==============================] - 45s 13ms/step - loss: 1.1847e-04 - acc: 1.0000 - val_loss: 6.1334e-05 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "3565/3565 [==============================] - 44s 12ms/step - loss: 1.3182e-04 - acc: 1.0000 - val_loss: 5.4589e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4d09f1048>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(train_data, labels_train,\n",
    "#             batch_size=64,\n",
    "#             epochs=5,\n",
    "#             validation_data=(test_data, labels_test))\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "            batch_size=64,\n",
    "            epochs=5,\n",
    "            validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.3949499e-01, 4.6050492e-01],\n",
       "       [5.3976852e-01, 4.6023145e-01],\n",
       "       [5.4747850e-01, 4.5252153e-01],\n",
       "       ...,\n",
       "       [5.4092926e-01, 4.5907077e-01],\n",
       "       [3.6320067e-04, 9.9963677e-01],\n",
       "       [5.3984576e-01, 4.6015424e-01]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted=model.predict(test_data)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115/1115 [==============================] - 4s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08230450725331151, 0.9874439467229116]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [0.07058866604882806, 0.9874439467229116]\n",
    "model.evaluate(test_data, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.98950682 0.96296296]\n",
      "recall: [0.99367756 0.93975904]\n",
      "fscore: [0.9915878  0.95121951]\n",
      "support: [949 166]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       949\n",
      "          1       0.96      0.94      0.95       166\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test,predicted.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,predicted.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now define our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SIMPLERNN model.\n",
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/5\n",
      "4457/4457 [==============================] - 36s 8ms/step - loss: 0.3558 - acc: 0.9215 - val_loss: 0.1980 - val_acc: 0.9659\n",
      "Epoch 2/5\n",
      "4457/4457 [==============================] - 35s 8ms/step - loss: 0.0945 - acc: 0.9892 - val_loss: 0.1364 - val_acc: 0.9668\n",
      "Epoch 3/5\n",
      "4457/4457 [==============================] - 35s 8ms/step - loss: 0.0388 - acc: 0.9964 - val_loss: 0.1192 - val_acc: 0.9668\n",
      "Epoch 4/5\n",
      "4457/4457 [==============================] - 33s 7ms/step - loss: 0.0207 - acc: 0.9987 - val_loss: 0.1106 - val_acc: 0.9686\n",
      "Epoch 5/5\n",
      "4457/4457 [==============================] - 34s 8ms/step - loss: 0.0130 - acc: 0.9991 - val_loss: 0.1112 - val_acc: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4da152a58>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import library\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "\n",
    "#model training\n",
    "print('Training SIMPLERNN model.')\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    "          EMBEDDING_DIM,\n",
    "          input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=16,\n",
    "          epochs=5,\n",
    "          validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97940284, 0.02059711],\n",
       "       [0.9901631 , 0.00983696],\n",
       "       [0.86733055, 0.13266945],\n",
       "       ...,\n",
       "       [0.9935961 , 0.00640392],\n",
       "       [0.01116436, 0.98883563],\n",
       "       [0.9942239 , 0.00577611]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.96817248 0.95744681]\n",
      "recall: [0.99367756 0.81325301]\n",
      "fscore: [0.98075923 0.87947883]\n",
      "support: [949 166]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       949\n",
      "          1       0.96      0.81      0.88       166\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test,predicted_Srnn.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,predicted_Srnn.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And here is our Long Short-Term Memory (LSTM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\workspace\\python\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"relu\", return_sequences=True, units=16, recurrent_activation=\"hard_sigmoid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/5\n",
      "4457/4457 [==============================] - 79s 18ms/step - loss: 0.1239 - acc: 0.9589 - val_loss: 0.0499 - val_acc: 0.9830\n",
      "Epoch 2/5\n",
      "4457/4457 [==============================] - 77s 17ms/step - loss: 0.0116 - acc: 0.9971 - val_loss: 0.1001 - val_acc: 0.9749\n",
      "Epoch 3/5\n",
      "4457/4457 [==============================] - 76s 17ms/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0555 - val_acc: 0.9892\n",
      "Epoch 4/5\n",
      "4457/4457 [==============================] - 75s 17ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0777 - val_acc: 0.9848\n",
      "Epoch 5/5\n",
      "4457/4457 [==============================] - 72s 16ms/step - loss: 4.2686e-04 - acc: 0.9998 - val_loss: 0.0742 - val_acc: 0.9874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4dcabc128>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training\n",
    "print('Training LSTM model.')\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    "          EMBEDDING_DIM,\n",
    "          input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(output_dim=16,\n",
    "               activation='relu', \n",
    "               inner_activation='hard_sigmoid',\n",
    "               return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=16,\n",
    "          epochs=5,\n",
    "          validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99725163e-01, 2.74797145e-04],\n",
       "       [9.98966575e-01, 1.03342067e-03],\n",
       "       [9.99999881e-01, 1.02845455e-07],\n",
       "       ...,\n",
       "       [9.99988198e-01, 1.17637801e-05],\n",
       "       [5.07229725e-09, 1.00000000e+00],\n",
       "       [9.99894619e-01, 1.05330182e-04]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.98748697 0.98717949]\n",
      "recall: [0.99789252 0.92771084]\n",
      "fscore: [0.99266247 0.95652174]\n",
      "support: [949 166]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       949\n",
      "          1       0.99      0.93      0.96       166\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test,predicted_lstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,predicted_lstm.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Finally, let’s see what is Bidirectional LSTM and implement the same.\n",
    "\n",
    "As we know, LSTM preserves information from inputs using the\n",
    "hidden state. In bidirectional LSTMs, inputs are fed in two ways: one\n",
    "from previous to future and the other going backward from future to\n",
    "past, helping in learning future representation as well. Bidirectional\n",
    "LSTMs are known for producing very good results as they are capable of\n",
    "understanding the context better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bidirectional LSTM model.\n",
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/3\n",
      "4457/4457 [==============================] - 130s 29ms/step - loss: 0.1466 - acc: 0.9520 - val_loss: 0.0480 - val_acc: 0.9865\n",
      "Epoch 2/3\n",
      "4457/4457 [==============================] - 128s 29ms/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0594 - val_acc: 0.9874\n",
      "Epoch 3/3\n",
      "4457/4457 [==============================] - 124s 28ms/step - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0706 - val_acc: 0.9874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b4e4f29d30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training\n",
    "print('Training Bidirectional LSTM model.')\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    "          EMBEDDING_DIM,\n",
    "          input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=True,dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\",kernel_initializer = \"glorot_uniform\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer='adam',metrics = ['accuracy'])\n",
    "model.fit(train_data, labels_train,\n",
    "          batch_size=16,\n",
    "          epochs=3,\n",
    "          validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9998069e-01, 1.9336485e-05],\n",
       "       [9.9972147e-01, 2.7848408e-04],\n",
       "       [9.9995434e-01, 4.5624503e-05],\n",
       "       ...,\n",
       "       [9.9998879e-01, 1.1160270e-05],\n",
       "       [1.5796587e-04, 9.9984205e-01],\n",
       "       [9.9994087e-01, 5.9070258e-05]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_blstm=model.predict(test_data)\n",
    "predicted_blstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.98647242 0.99350649]\n",
      "recall: [0.99894626 0.92168675]\n",
      "fscore: [0.99267016 0.95625   ]\n",
      "support: [949 166]\n",
      "############################\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99       949\n",
      "          1       0.99      0.92      0.96       166\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test,predicted_blstm.round())\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,predicted_blstm.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
